{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling1D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_len = 20\n",
    "cutoff = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    X_train = [\n",
    "        \"Building a bike\",\n",
    "        \"Building a robot\",\n",
    "        \"Building a wall\",\n",
    "        \"Writing a program\",\n",
    "        \"Writing an article\",\n",
    "        \"Writing a play\",\n",
    "        \"Learning to draw\",\n",
    "        \"Learning to paint\",\n",
    "        \"Learning to cook\",\n",
    "        \"Learning Klingon\",\n",
    "        \"Learning Java\",\n",
    "        \"Exploring math\",\n",
    "        \"Exploring art\",\n",
    "        \"Exploring mindfulness\"\n",
    "    ]\n",
    "    \n",
    "    # for the purpose of this model, I am disregarding the capitalization of titles\n",
    "    X_train = [i.lower() for i in X_train]\n",
    "    \n",
    "    # changes the token pattern to include single character words\n",
    "    word_vector = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "    \n",
    "    term_freq = word_vector.fit_transform(X_train)\n",
    "    word_index = word_vector.vocabulary_\n",
    "    word_index[\"PADDING\"] = len(word_vector.vocabulary_)\n",
    "    reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "    \n",
    "    # dimensions should be 14 (number of titles/data points) by 3 (words/channels) by 22 (vocabulary/classes)\n",
    "    \n",
    "    def split_words(string):\n",
    "        return string.split(\" \")\n",
    "\n",
    "    def pad_data(data, cutoff):\n",
    "        for i in data:\n",
    "            while len(i) < cutoff:\n",
    "                i.append(\"PADDING\")\n",
    "\n",
    "    def index_words(word_list, word_index):\n",
    "        return [word_index[i] for i in word_list]\n",
    "\n",
    "    X_train = list(map(split_words, X_train))\n",
    "\n",
    "    pad_data(X_train, cutoff)\n",
    "\n",
    "    X_train = [index_words(i, word_index) for i in X_train]\n",
    "#     return to_categorical(X_train, num_classes=22)#[i] for i in range(len(X_train)))\n",
    "    return(\n",
    "        np.array([np.transpose(to_categorical(X_train, num_classes=22)[i]) for i in range(len(X_train))]),\n",
    "        reverse_word_index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "X_train, vocab = process_data()\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    '''\n",
    "    Put together a CNN that will return a single confidence output.\n",
    "    \n",
    "    returns: the model object\n",
    "    '''\n",
    "    \n",
    "    # dimensions: rows = list of words (22), columns/channels = # of words per title (3)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, strides=2, input_shape=(22, 3), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    '''\n",
    "    Put together a model that takes in one-dimensional noise and outputs two-dimensional\n",
    "    data representing a three-word phrase.\n",
    "    \n",
    "    returns: the model object\n",
    "    '''\n",
    "    \n",
    "    # note to self: trailing commas are used for single-element tuples\n",
    "    noise_shape = (noise_len,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(22 * 3, activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape((22, 3)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(Conv1D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(3, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined():\n",
    "    '''\n",
    "    Puts together a model that combines the discriminator and generator models.\n",
    "    \n",
    "    returns: the generator, discriminator, and combined model objects\n",
    "    '''\n",
    "    \n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(loss='binary_crossentropy', \n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Build and compile the generator\n",
    "    generator = build_generator()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # The generator takes noise as input and generates images\n",
    "    noise = Input(shape=(noise_len,))\n",
    "    title = generator(noise)\n",
    "    \n",
    "    \n",
    "    # For the combined model we will only train the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated images as input and determines validity\n",
    "    valid = discriminator(title)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity \n",
    "    combined = Model(inputs=noise, outputs=valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator, discriminator, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_titles(generator, epoch, vocab):\n",
    "    '''\n",
    "    Has the generator create and save new Quest titles, so God help us all.\n",
    "    \n",
    "    inputs:\n",
    "        generator: the generator model object returned by build_combined\n",
    "        epoch: the epoch number (but can be anything that can be represented as a string)\n",
    "        vocab: the mapping of numbers to words\n",
    "    \n",
    "    returns: None\n",
    "    '''\n",
    "    titles = 1\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (titles, noise_len))\n",
    "    gen_title = generator.predict(noise)\n",
    "    \n",
    "    # chooses the word with the highest weight\n",
    "    gen_title = [np.argmax([j[i] for j in gen_title[0]]) for i in range(3)]\n",
    "    \n",
    "    # map words to numbers\n",
    "    gen_title = \" \".join([vocab[i] for i in gen_title])\n",
    "    \n",
    "    file=open('titles/titles_{}.txt'.format(epoch),\"w+\")\n",
    "    file.write(gen_title)\n",
    "    print(gen_title)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, combined, data, vocab, epochs, batch_size=128, save_interval=50):\n",
    "    '''\n",
    "    Trains all model objects\n",
    "    \n",
    "    generator: the generator model object returned by build_combined\n",
    "    discriminator: the discriminator model object returned by build_combined\n",
    "    combined: the combined model object returned by build_combined\n",
    "    epochs: integer, the number of epochs to train for\n",
    "    batch_size: integer, the number of training samples to use at a time\n",
    "    save_interval: integer, will generate and save images when the current epoch % save_interval is 0\n",
    "    \n",
    "    returns: None\n",
    "    '''\n",
    "\n",
    "    # Load the dataset\n",
    "    X_train = data\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        titles = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a half batch of new images\n",
    "        noise = np.random.normal(0, 1, (half_batch, noise_len))\n",
    "        gen_titles = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(titles, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_titles, np.zeros((half_batch, 1)))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_len))\n",
    "        # Train the generator (wants discriminator to mistake images as real)\n",
    "        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "           \n",
    "        # If at save interval => save generated image samples and plot progress\n",
    "        if epoch % save_interval == 0:\n",
    "            # Plot the progress\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            print (\"{} [D loss: {}, acc.: {:.2%}] [G loss: {}]\".format(epoch, d_loss[0], d_loss[1], g_loss))\n",
    "            save_titles(generator, epoch, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, discriminator, combined = build_combined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.8421581983566284, acc.: 40.62%] [G loss: 0.6872787475585938]\n",
      "klingon cook building\n",
      "50 [D loss: 0.35807278752326965, acc.: 78.12%] [G loss: 1.859682559967041]\n",
      "building building robot\n",
      "100 [D loss: 0.25104019045829773, acc.: 96.88%] [G loss: 2.043210744857788]\n",
      "mindfulness play to\n",
      "150 [D loss: 0.0837550163269043, acc.: 100.00%] [G loss: 2.999795436859131]\n",
      "to learning learning\n",
      "200 [D loss: 0.11396276950836182, acc.: 96.88%] [G loss: 3.410893678665161]\n",
      "math program paint\n",
      "250 [D loss: 0.08014360815286636, acc.: 100.00%] [G loss: 4.34560489654541]\n",
      "writing math cook\n",
      "300 [D loss: 0.11298981308937073, acc.: 96.88%] [G loss: 5.073624610900879]\n",
      "exploring a article\n",
      "350 [D loss: 0.0650494173169136, acc.: 100.00%] [G loss: 5.997878551483154]\n",
      "learning klingon klingon\n",
      "400 [D loss: 0.1469355821609497, acc.: 93.75%] [G loss: 5.929841041564941]\n",
      "PADDING to robot\n",
      "450 [D loss: 0.0684782937169075, acc.: 100.00%] [G loss: 5.444269180297852]\n",
      "writing a bike\n",
      "500 [D loss: 0.04744041711091995, acc.: 96.88%] [G loss: 4.999983787536621]\n",
      "learning cook mindfulness\n",
      "550 [D loss: 0.03599746525287628, acc.: 100.00%] [G loss: 5.526270389556885]\n",
      "writing a PADDING\n",
      "600 [D loss: 0.06742291897535324, acc.: 93.75%] [G loss: 6.444616794586182]\n",
      "wall to PADDING\n",
      "650 [D loss: 0.014666673727333546, acc.: 100.00%] [G loss: 5.567352294921875]\n",
      "learning to java\n",
      "700 [D loss: 0.016149338334798813, acc.: 100.00%] [G loss: 5.838268280029297]\n",
      "PADDING a wall\n",
      "750 [D loss: 0.05180824175477028, acc.: 96.88%] [G loss: 5.4431633949279785]\n",
      "building to an\n",
      "800 [D loss: 0.0843685120344162, acc.: 96.88%] [G loss: 6.37786865234375]\n",
      "bike to a\n",
      "850 [D loss: 0.019063841551542282, acc.: 100.00%] [G loss: 6.443211555480957]\n",
      "article art PADDING\n",
      "900 [D loss: 0.009841140359640121, acc.: 100.00%] [G loss: 7.598237991333008]\n",
      "learning math paint\n",
      "950 [D loss: 0.018047446385025978, acc.: 100.00%] [G loss: 6.799771308898926]\n",
      "program a PADDING\n",
      "1000 [D loss: 0.02119350992143154, acc.: 100.00%] [G loss: 6.8087968826293945]\n",
      "building art bike\n",
      "1050 [D loss: 0.048546627163887024, acc.: 96.88%] [G loss: 6.861637115478516]\n",
      "learning java PADDING\n",
      "1100 [D loss: 0.018871016800403595, acc.: 100.00%] [G loss: 6.965543746948242]\n",
      "PADDING an article\n",
      "1150 [D loss: 0.014778041280806065, acc.: 100.00%] [G loss: 6.496408462524414]\n",
      "exploring a PADDING\n",
      "1200 [D loss: 0.039497390389442444, acc.: 100.00%] [G loss: 7.81453800201416]\n",
      "bike a PADDING\n",
      "1250 [D loss: 0.020879622548818588, acc.: 100.00%] [G loss: 6.650846004486084]\n",
      "writing a wall\n",
      "1300 [D loss: 0.013176755048334599, acc.: 100.00%] [G loss: 7.296144008636475]\n",
      "learning a PADDING\n",
      "1350 [D loss: 0.05302628502249718, acc.: 96.88%] [G loss: 7.064169883728027]\n",
      "art to PADDING\n",
      "1400 [D loss: 0.09632112085819244, acc.: 96.88%] [G loss: 5.494662284851074]\n",
      "learning to PADDING\n",
      "1450 [D loss: 0.05054834485054016, acc.: 100.00%] [G loss: 7.899580001831055]\n",
      "writing a PADDING\n",
      "1500 [D loss: 0.04580116271972656, acc.: 96.88%] [G loss: 7.2355146408081055]\n",
      "learning to wall\n",
      "1550 [D loss: 0.005719216074794531, acc.: 100.00%] [G loss: 7.394015312194824]\n",
      "building a bike\n",
      "1600 [D loss: 0.06512978672981262, acc.: 96.88%] [G loss: 8.206708908081055]\n",
      "learning klingon PADDING\n",
      "1650 [D loss: 0.1764400154352188, acc.: 93.75%] [G loss: 6.478412628173828]\n",
      "mindfulness to mindfulness\n",
      "1700 [D loss: 0.05826101079583168, acc.: 96.88%] [G loss: 9.602455139160156]\n",
      "learning klingon PADDING\n",
      "1750 [D loss: 0.009362855926156044, acc.: 100.00%] [G loss: 7.182109832763672]\n",
      "learning a PADDING\n",
      "1800 [D loss: 0.01157536543905735, acc.: 100.00%] [G loss: 7.15797233581543]\n",
      "exploring a PADDING\n",
      "1850 [D loss: 0.028286464512348175, acc.: 100.00%] [G loss: 7.307021141052246]\n",
      "learning klingon PADDING\n",
      "1900 [D loss: 0.05515151470899582, acc.: 96.88%] [G loss: 9.384017944335938]\n",
      "exploring a PADDING\n",
      "1950 [D loss: 0.23572520911693573, acc.: 93.75%] [G loss: 8.638745307922363]\n",
      "writing art program\n",
      "2000 [D loss: 0.003443480469286442, acc.: 100.00%] [G loss: 10.429092407226562]\n",
      "exploring math PADDING\n",
      "2050 [D loss: 0.048005301505327225, acc.: 96.88%] [G loss: 8.467061996459961]\n",
      "learning a PADDING\n",
      "2100 [D loss: 0.06798003613948822, acc.: 96.88%] [G loss: 7.71405029296875]\n",
      "learning klingon PADDING\n",
      "2150 [D loss: 0.01029394194483757, acc.: 100.00%] [G loss: 9.279914855957031]\n",
      "writing art PADDING\n",
      "2200 [D loss: 0.012677657417953014, acc.: 100.00%] [G loss: 9.853638648986816]\n",
      "building a bike\n",
      "2250 [D loss: 0.044205162674188614, acc.: 96.88%] [G loss: 8.991127014160156]\n",
      "exploring math java\n",
      "2300 [D loss: 0.007536032237112522, acc.: 100.00%] [G loss: 9.844477653503418]\n",
      "writing math cook\n",
      "2350 [D loss: 0.041088275611400604, acc.: 100.00%] [G loss: 9.40521240234375]\n",
      "exploring to program\n",
      "2400 [D loss: 0.043535009026527405, acc.: 100.00%] [G loss: 7.19580078125]\n",
      "learning art PADDING\n",
      "2450 [D loss: 0.04139622673392296, acc.: 96.88%] [G loss: 8.54807186126709]\n",
      "learning to wall\n",
      "2500 [D loss: 0.0027877860702574253, acc.: 100.00%] [G loss: 9.969231605529785]\n",
      "writing a PADDING\n",
      "2550 [D loss: 0.018583614379167557, acc.: 100.00%] [G loss: 10.44668960571289]\n",
      "exploring an article\n",
      "2600 [D loss: 0.010097934864461422, acc.: 100.00%] [G loss: 8.926748275756836]\n",
      "an a PADDING\n",
      "2650 [D loss: 0.005252111237496138, acc.: 100.00%] [G loss: 10.162138938903809]\n",
      "learning a PADDING\n",
      "2700 [D loss: 0.006498072296380997, acc.: 100.00%] [G loss: 8.665352821350098]\n",
      "building an PADDING\n",
      "2750 [D loss: 0.021534480154514313, acc.: 100.00%] [G loss: 9.762533187866211]\n",
      "learning a PADDING\n",
      "2800 [D loss: 0.0008565994212403893, acc.: 100.00%] [G loss: 10.139892578125]\n",
      "learning to PADDING\n",
      "2850 [D loss: 0.02231963351368904, acc.: 100.00%] [G loss: 9.557499885559082]\n",
      "exploring art PADDING\n",
      "2900 [D loss: 0.0008826644043438137, acc.: 100.00%] [G loss: 7.908733367919922]\n",
      "writing a program\n",
      "2950 [D loss: 0.008615411818027496, acc.: 100.00%] [G loss: 8.693477630615234]\n",
      "exploring to robot\n",
      "3000 [D loss: 0.00813338439911604, acc.: 100.00%] [G loss: 8.811908721923828]\n",
      "building a PADDING\n",
      "3050 [D loss: 0.02056017518043518, acc.: 100.00%] [G loss: 9.223749160766602]\n",
      "learning klingon PADDING\n",
      "3100 [D loss: 0.0018085668561980128, acc.: 100.00%] [G loss: 10.384416580200195]\n",
      "exploring to bike\n",
      "3150 [D loss: 0.008312784135341644, acc.: 100.00%] [G loss: 11.151442527770996]\n",
      "learning math PADDING\n",
      "3200 [D loss: 0.008261959068477154, acc.: 100.00%] [G loss: 8.716302871704102]\n",
      "building a PADDING\n",
      "3250 [D loss: 0.011586084961891174, acc.: 100.00%] [G loss: 9.612882614135742]\n",
      "learning java PADDING\n",
      "3300 [D loss: 0.0011457223445177078, acc.: 100.00%] [G loss: 10.729217529296875]\n",
      "learning to PADDING\n",
      "3350 [D loss: 0.005366138182580471, acc.: 100.00%] [G loss: 10.390549659729004]\n",
      "exploring to article\n",
      "3400 [D loss: 0.011496719904243946, acc.: 100.00%] [G loss: 10.462942123413086]\n",
      "writing a an\n",
      "3450 [D loss: 0.020835712552070618, acc.: 100.00%] [G loss: 10.189717292785645]\n",
      "learning to paint\n",
      "3500 [D loss: 0.021072616800665855, acc.: 100.00%] [G loss: 10.330602645874023]\n",
      "learning a PADDING\n",
      "3550 [D loss: 0.037223342806100845, acc.: 96.88%] [G loss: 10.015581130981445]\n",
      "exploring a PADDING\n",
      "3600 [D loss: 0.01953127607703209, acc.: 100.00%] [G loss: 8.407157897949219]\n",
      "learning to paint\n",
      "3650 [D loss: 0.00316831236705184, acc.: 100.00%] [G loss: 9.391043663024902]\n",
      "building a PADDING\n",
      "3700 [D loss: 0.024661269038915634, acc.: 100.00%] [G loss: 9.279603958129883]\n",
      "writing an PADDING\n",
      "3750 [D loss: 0.004621740896254778, acc.: 100.00%] [G loss: 10.35183334350586]\n",
      "exploring java PADDING\n",
      "3800 [D loss: 0.055278804153203964, acc.: 96.88%] [G loss: 8.653694152832031]\n",
      "learning to draw\n",
      "3850 [D loss: 0.007456440478563309, acc.: 100.00%] [G loss: 13.071314811706543]\n",
      "exploring a PADDING\n",
      "3900 [D loss: 0.023548884317278862, acc.: 100.00%] [G loss: 10.421018600463867]\n",
      "learning java draw\n",
      "3950 [D loss: 0.013902019709348679, acc.: 100.00%] [G loss: 9.818414688110352]\n",
      "building a PADDING\n",
      "4000 [D loss: 0.00043436529813334346, acc.: 100.00%] [G loss: 9.716224670410156]\n",
      "exploring to draw\n"
     ]
    }
   ],
   "source": [
    "train(generator, discriminator, combined, X_train, vocab, epochs=4001, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6New",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
