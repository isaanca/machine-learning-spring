{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from [https://gist.github.com/EderSantana/c7222daa328f0e885093](https://gist.github.com/EderSantana/c7222daa328f0e885093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To be able to run the animation below, make sure you have the latest version of matplotlib, by running `pip3 install matplotlib --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import IPython.display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the game environment and replay classes\n",
    "The idea in this Catch game is that there is fruit falling, and the user gets to move a basket so that they catch the fruit. If they catch it, they win and the game is over. If they miss it, they lose and the game is over. We are trying to teach the computer to play this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        '''        \n",
    "        Initializes internal state.\n",
    "        '''\n",
    "        self.grid_size = grid_size\n",
    "        self.min_basket_center = 1\n",
    "        self.max_basket_center = self.grid_size-2\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Moves basket according to action. Moves fruit down. Updates state to reflect these movements\n",
    "        '''\n",
    "        if action == 0:  # left\n",
    "            movement = -1\n",
    "        elif action == 1:  # stay\n",
    "            movement = 0\n",
    "        elif action == 2: # right\n",
    "            movement = 1\n",
    "        else:\n",
    "            raise Exception('Invalid action {}'.format(action))\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        # move the basket unless this would move it off the edge of the grid\n",
    "        new_basket_center = min(max(self.min_basket_center, basket_center + movement), self.max_basket_center)\n",
    "        # move fruit down\n",
    "        fruit_y += 1\n",
    "        out = np.asarray([fruit_x, fruit_y, new_basket_center])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        '''\n",
    "        Returns a 2D numpy array with 1s (white squares) at the locations of the fruit and basket and\n",
    "        0s (black squares) everywhere else.\n",
    "        '''\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        canvas = np.zeros(im_size)\n",
    "        \n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        canvas[fruit_y, fruit_x] = 1  # draw fruit\n",
    "        canvas[-1, basket_center-1:basket_center + 2] = 1  # draw 3-pixel basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        '''\n",
    "        Returns 1 if the fruit was caught, -1 if it was dropped, and 0 if it is still in the air.\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        if fruit_y == self.grid_size-1:\n",
    "            if abs(fruit_x - basket_center) <= 1:\n",
    "                return 1 # it caught the fruit\n",
    "            else:\n",
    "                return -1 # it dropped the fruit\n",
    "        else:\n",
    "            return 0 # the fruit is still in the air\n",
    "\n",
    "    def observe(self):\n",
    "        '''\n",
    "        Returns the current canvas, as a 1D array.\n",
    "        '''\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, policy):\n",
    "        '''\n",
    "        Input: policy (a 10x10x10 array of actions for each possible state of fruit_x, fruit_y, and basket_center\n",
    "        with 0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Returns:\n",
    "            current canvas (as a 1D array)\n",
    "            reward received after this action\n",
    "            True if game is over and False otherwise\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        action = policy[fruit_x][fruit_y][basket_center]\n",
    "        if (action not in [0,1,2]):\n",
    "            print(\"BAD\")\n",
    "#             print(policy)\n",
    "#         print(\"Policy:\")\n",
    "#         print(policy)\n",
    "#         print(\"Action:\")\n",
    "#         print(action)\n",
    "        self._update_state(action)\n",
    "        observation = self.observe()\n",
    "        reward = self._get_reward()\n",
    "        game_over = (reward != 0) # if the reward is zero, the fruit is still in the air\n",
    "        return observation, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Updates internal state\n",
    "            fruit in a random column in the top row\n",
    "            basket center in a random column\n",
    "        '''\n",
    "        fruit_x = random.randint(0, self.grid_size-1)\n",
    "        fruit_y = 0\n",
    "        basket_center = random.randint(self.min_basket_center, self.max_basket_center)\n",
    "        self.state = np.asarray([fruit_x, fruit_y, basket_center])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''\n",
    "        Input:\n",
    "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
    "            game_over: boolean\n",
    "        Add the states and game over to the internal memory array. If the array is longer than\n",
    "        self.max_memory, drop the oldest memory\n",
    "        '''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        '''\n",
    "        Randomly chooses batch_size memories, possibly repeating.\n",
    "        For each of these memories, updates the models current best guesses about the value of taking a\n",
    "            certain action from the starting state, based on the reward received and the model's current\n",
    "            estimate of how valuable the new state is.\n",
    "        '''\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1] # the number of possible actions\n",
    "        env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
    "        input_size = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((input_size, env_dim))\n",
    "        targets = np.zeros((input_size, num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
    "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # Set the input to the state that was observed in the game before an action was taken\n",
    "            inputs[i:i+1] = starting_observation\n",
    "            \n",
    "            # Start with the model's current best guesses about the value of taking each action from this state\n",
    "            targets[i] = model.predict(starting_observation)[0]\n",
    "            \n",
    "            # Now we need to update the value of the action that was taken                      \n",
    "            if game_over: \n",
    "                # if the game is over, give the actual reward received\n",
    "                targets[i, action_taken] = reward_received\n",
    "            else:\n",
    "                # if the game is not over, give the reward received (always zero in this particular game)\n",
    "                # plus the maximum reward predicted for state we got to by taking this action (with a discount)\n",
    "                Q_sa = np.max(model.predict(new_observation)[0])\n",
    "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions for creating, training, and visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epsilon = .1  # probability of exploration (choosing a random action instead of the current best one)\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 50\n",
    "grid_size = 10\n",
    "\n",
    "def run_episode(env, policy, grid_size=10, episode_len=100):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        fruit_x, fruit_y, basket_center = env.state\n",
    "        obs, reward, done = env.act(policy)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            # print('Episode finished after {} timesteps.'.format(t+1))\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    return np.random.choice(3, size=((10,10,10)))\n",
    "\n",
    "def crossover(policy1, policy2, grid_size=10):\n",
    "    new_policy = policy1.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            for k in range(10):\n",
    "                rand = np.random.uniform()\n",
    "                if rand > 0.5:\n",
    "                    new_policy[i][j][k] = policy2[i][j][k]\n",
    "    return new_policy\n",
    "\n",
    "def mutation(policy, p=0.05):\n",
    "    new_policy = policy.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            for k in range(10):\n",
    "                rand = np.random.uniform()\n",
    "                if rand < p:\n",
    "                    new_policy[i] = np.random.choice(3)\n",
    "    return new_policy\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "     Returns three initialized objects: the model, the environment, and the replay.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.2), \"mse\")\n",
    "\n",
    "    # Define environment/game\n",
    "    env = Catch()\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "    \n",
    "    return model, env, exp_replay\n",
    "\n",
    "def create_animation(model, env, num_games):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model and env objects as returned from build_model\n",
    "        num_games: integer, the number of games to be included in the animation\n",
    "        \n",
    "    Returns: a matplotlib animation object\n",
    "    '''\n",
    "    # Animation code from \n",
    "    # https://matplotlib.org/examples/animation/dynamic_image.html\n",
    "    # https://stackoverflow.com/questions/35532498/animation-in-ipython-notebook/46878531#46878531\n",
    "    \n",
    "    # First, play the games and collect all of the images for each observed state\n",
    "    observations = []\n",
    "    for _ in range(num_games):\n",
    "        env.reset()\n",
    "        observation = env.observe()\n",
    "        observations.append(observation)\n",
    "        game_over = False\n",
    "        while game_over == False:\n",
    "            q = model.predict(observation)\n",
    "            action = np.argmax(q[0])\n",
    "            \n",
    "            # apply action, get rewards and new state\n",
    "            observation, reward, game_over = env.act(policy)\n",
    "            observations.append(observation)\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    image = plt.imshow(np.zeros((grid_size, grid_size)),interpolation='none', cmap='gray', animated=True, vmin=0, vmax=1)\n",
    "    \n",
    "    def animate(observation):\n",
    "        image.set_array(observation.reshape((grid_size, grid_size)))\n",
    "        return [image]\n",
    "   \n",
    "    animation = matplotlib.animation.FuncAnimation(fig, animate, frames=observations, blit=True, )\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 : max score = 0.08\n",
      "Generation 2 : max score = 0.26\n",
      "Generation 3 : max score = 0.30\n",
      "Generation 4 : max score = 0.40\n",
      "Generation 5 : max score = 0.28\n",
      "Generation 6 : max score = 0.38\n",
      "Generation 7 : max score = 0.36\n",
      "Generation 8 : max score = 0.38\n",
      "Generation 9 : max score = 0.34\n",
      "Generation 10 : max score = 0.38\n",
      "Generation 11 : max score = 0.30\n",
      "Generation 12 : max score = 0.34\n",
      "Generation 13 : max score = 0.40\n",
      "Generation 14 : max score = 0.28\n",
      "Generation 15 : max score = 0.28\n",
      "Generation 16 : max score = 0.30\n",
      "Generation 17 : max score = 0.36\n",
      "Generation 18 : max score = 0.38\n",
      "Generation 19 : max score = 0.44\n",
      "Generation 20 : max score = 0.44\n",
      "Generation 21 : max score = 0.34\n",
      "Generation 22 : max score = 0.44\n",
      "Generation 23 : max score = 0.40\n",
      "Generation 24 : max score = 0.40\n",
      "Generation 25 : max score = 0.32\n",
      "Generation 26 : max score = 0.36\n",
      "Generation 27 : max score = 0.40\n",
      "Generation 28 : max score = 0.40\n",
      "Generation 29 : max score = 0.32\n",
      "Generation 30 : max score = 0.38\n",
      "Generation 31 : max score = 0.38\n",
      "Generation 32 : max score = 0.38\n",
      "Generation 33 : max score = 0.34\n",
      "Generation 34 : max score = 0.40\n",
      "Generation 35 : max score = 0.38\n",
      "Generation 36 : max score = 0.34\n",
      "Generation 37 : max score = 0.32\n",
      "Generation 38 : max score = 0.38\n",
      "Generation 39 : max score = 0.54\n",
      "Generation 40 : max score = 0.42\n",
      "Generation 41 : max score = 0.56\n",
      "Generation 42 : max score = 0.42\n",
      "Generation 43 : max score = 0.44\n",
      "Generation 44 : max score = 0.32\n",
      "Generation 45 : max score = 0.56\n",
      "Generation 46 : max score = 0.54\n",
      "Generation 47 : max score = 0.42\n",
      "Generation 48 : max score = 0.60\n",
      "Generation 49 : max score = 0.46\n",
      "Generation 50 : max score = 0.44\n",
      "Generation 51 : max score = 0.50\n",
      "Generation 52 : max score = 0.48\n",
      "Generation 53 : max score = 0.34\n",
      "Generation 54 : max score = 0.50\n",
      "Generation 55 : max score = 0.38\n",
      "Generation 56 : max score = 0.44\n",
      "Generation 57 : max score = 0.60\n",
      "Generation 58 : max score = 0.58\n",
      "Generation 59 : max score = 0.40\n",
      "Generation 60 : max score = 0.38\n",
      "Generation 61 : max score = 0.42\n",
      "Generation 62 : max score = 0.46\n",
      "Generation 63 : max score = 0.42\n",
      "Generation 64 : max score = 0.42\n",
      "Generation 65 : max score = 0.58\n",
      "Generation 66 : max score = 0.46\n",
      "Generation 67 : max score = 0.46\n",
      "Generation 68 : max score = 0.58\n",
      "Generation 69 : max score = 0.46\n",
      "Generation 70 : max score = 0.42\n",
      "Generation 71 : max score = 0.52\n",
      "Generation 72 : max score = 0.48\n",
      "Generation 73 : max score = 0.52\n",
      "Generation 74 : max score = 0.42\n",
      "Generation 75 : max score = 0.40\n",
      "Generation 76 : max score = 0.38\n",
      "Generation 77 : max score = 0.34\n",
      "Generation 78 : max score = 0.38\n",
      "Generation 79 : max score = 0.44\n",
      "Generation 80 : max score = 0.32\n",
      "Generation 81 : max score = 0.44\n",
      "Generation 82 : max score = 0.40\n",
      "Generation 83 : max score = 0.34\n",
      "Generation 84 : max score = 0.34\n",
      "Generation 85 : max score = 0.30\n",
      "Generation 86 : max score = 0.40\n",
      "Generation 87 : max score = 0.48\n",
      "Generation 88 : max score = 0.32\n",
      "Generation 89 : max score = 0.46\n",
      "Generation 90 : max score = 0.54\n",
      "Generation 91 : max score = 0.44\n",
      "Generation 92 : max score = 0.36\n",
      "Generation 93 : max score = 0.44\n",
      "Generation 94 : max score = 0.52\n",
      "Generation 95 : max score = 0.38\n",
      "Generation 96 : max score = 0.44\n",
      "Generation 97 : max score = 0.42\n",
      "Generation 98 : max score = 0.44\n",
      "Generation 99 : max score = 0.40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-363277865edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselect_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n\u001b[0;32m---> 27\u001b[0;31m         for _ in range(n_policy - 5)]\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmutated_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-363277865edc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselect_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n\u001b[0;32m---> 27\u001b[0;31m         for _ in range(n_policy - 5)]\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmutated_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4fe037b03f21>\u001b[0m in \u001b[0;36mcrossover\u001b[0;34m(policy1, policy2, grid_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mnew_policy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(4904) # we kick bot\n",
    "np.random.seed(4904)\n",
    "model, env, exp_replay = build_model()\n",
    "## Policy search\n",
    "n_policy = 254\n",
    "n_steps = 649\n",
    "\n",
    "# note to self: the underscore can be used as a throwaway value when iterating through something,\n",
    "# like when you want to do something x times but don't care about the value of x\n",
    "policy_pop = [gen_random_policy() for _ in range(n_policy)]\n",
    "\n",
    "env.reset()\n",
    "env.state\n",
    "policy_pop\n",
    "test_policy = policy_pop[0]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    policy_scores = [evaluate_policy(env, p) for p in policy_pop]\n",
    "    print('Generation %d : max score = %0.2f' %(idx+1, max(policy_scores)))\n",
    "    policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "    elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "    policy_scores_non_negative = np.array(policy_scores)+1\n",
    "    select_probs = (np.array(policy_scores_non_negative)) / np.sum(policy_scores_non_negative)\n",
    "    child_set = [crossover(\n",
    "        policy_pop[np.random.choice(range(n_policy), p=select_probs)],\n",
    "        policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n",
    "        for _ in range(n_policy - 5)]\n",
    "    mutated_list = [mutation(p) for p in child_set]\n",
    "    policy_pop = elite_set\n",
    "    policy_pop += mutated_list\n",
    "policy_score = [evaluate_policy(env, p) for p in policy_pop]\n",
    "best_policy = policy_pop[np.argmax(policy_score)]\n",
    "\n",
    "print('Best policy score = %0.2f.'\n",
    "        %(np.max(policy_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
