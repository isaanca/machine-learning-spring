{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from [https://gist.github.com/EderSantana/c7222daa328f0e885093](https://gist.github.com/EderSantana/c7222daa328f0e885093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To be able to run the animation below, make sure you have the latest version of matplotlib, by running `pip3 install matplotlib --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import IPython.display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the game environment and replay classes\n",
    "The idea in this Catch game is that there is fruit falling, and the user gets to move a basket so that they catch the fruit. If they catch it, they win and the game is over. If they miss it, they lose and the game is over. We are trying to teach the computer to play this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        '''        \n",
    "        Initializes internal state.\n",
    "        '''\n",
    "        self.grid_size = grid_size\n",
    "        self.min_basket_center = 1\n",
    "        self.max_basket_center = self.grid_size-2\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Moves basket according to action. Moves fruit down. Updates state to reflect these movements\n",
    "        '''\n",
    "        if action == 0:  # left\n",
    "            movement = -1\n",
    "        elif action == 1:  # stay\n",
    "            movement = 0\n",
    "        elif action == 2: # right\n",
    "            movement = 1\n",
    "        else:\n",
    "            raise Exception('Invalid action {}'.format(action))\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        # move the basket unless this would move it off the edge of the grid\n",
    "        new_basket_center = min(max(self.min_basket_center, basket_center + movement), self.max_basket_center)\n",
    "        # move fruit down\n",
    "        fruit_y += 1\n",
    "        out = np.asarray([fruit_x, fruit_y, new_basket_center])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        '''\n",
    "        Returns a 2D numpy array with 1s (white squares) at the locations of the fruit and basket and\n",
    "        0s (black squares) everywhere else.\n",
    "        '''\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        canvas = np.zeros(im_size)\n",
    "        \n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        canvas[fruit_y, fruit_x] = 1  # draw fruit\n",
    "        canvas[-1, basket_center-1:basket_center + 2] = 1  # draw 3-pixel basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        '''\n",
    "        Returns 1 if the fruit was caught, -1 if it was dropped, and 0 if it is still in the air.\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        if fruit_y == self.grid_size-1:\n",
    "            if abs(fruit_x - basket_center) <= 1:\n",
    "                return 1 # it caught the fruit\n",
    "            else:\n",
    "                return -1 # it dropped the fruit\n",
    "        else:\n",
    "            return 0 # the fruit is still in the air\n",
    "\n",
    "    def observe(self):\n",
    "        '''\n",
    "        Returns the current canvas, as a 1D array.\n",
    "        '''\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, policy):\n",
    "        '''\n",
    "        Input: policy (a 10x10x10 array of actions for each possible state of fruit_x, fruit_y, and basket_center\n",
    "        with 0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Returns:\n",
    "            current canvas (as a 1D array)\n",
    "            reward received after this action\n",
    "            True if game is over and False otherwise\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        action = policy[fruit_x][fruit_y][basket_center]\n",
    "#         print(\"Policy:\")\n",
    "#         print(policy)\n",
    "#         print(\"Action:\")\n",
    "#         print(action)\n",
    "        self._update_state(action)\n",
    "        observation = self.observe()\n",
    "        reward = self._get_reward()\n",
    "        game_over = (reward != 0) # if the reward is zero, the fruit is still in the air\n",
    "        return observation, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Updates internal state\n",
    "            fruit in a random column in the top row\n",
    "            basket center in a random column\n",
    "        '''\n",
    "        fruit_x = random.randint(0, self.grid_size-1)\n",
    "        fruit_y = 0\n",
    "        basket_center = random.randint(self.min_basket_center, self.max_basket_center)\n",
    "        self.state = np.asarray([fruit_x, fruit_y, basket_center])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''\n",
    "        Input:\n",
    "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
    "            game_over: boolean\n",
    "        Add the states and game over to the internal memory array. If the array is longer than\n",
    "        self.max_memory, drop the oldest memory\n",
    "        '''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        '''\n",
    "        Randomly chooses batch_size memories, possibly repeating.\n",
    "        For each of these memories, updates the models current best guesses about the value of taking a\n",
    "            certain action from the starting state, based on the reward received and the model's current\n",
    "            estimate of how valuable the new state is.\n",
    "        '''\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1] # the number of possible actions\n",
    "        env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
    "        input_size = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((input_size, env_dim))\n",
    "        targets = np.zeros((input_size, num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
    "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # Set the input to the state that was observed in the game before an action was taken\n",
    "            inputs[i:i+1] = starting_observation\n",
    "            \n",
    "            # Start with the model's current best guesses about the value of taking each action from this state\n",
    "            targets[i] = model.predict(starting_observation)[0]\n",
    "            \n",
    "            # Now we need to update the value of the action that was taken                      \n",
    "            if game_over: \n",
    "                # if the game is over, give the actual reward received\n",
    "                targets[i, action_taken] = reward_received\n",
    "            else:\n",
    "                # if the game is not over, give the reward received (always zero in this particular game)\n",
    "                # plus the maximum reward predicted for state we got to by taking this action (with a discount)\n",
    "                Q_sa = np.max(model.predict(new_observation)[0])\n",
    "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions for creating, training, and visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epsilon = .1  # probability of exploration (choosing a random action instead of the current best one)\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 50\n",
    "grid_size = 10\n",
    "\n",
    "def run_episode(env, policy, grid_size=10, episode_len=100):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        fruit_x, fruit_y, basket_center = env.state\n",
    "        obs, reward, done = env.act(policy)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            # print('Episode finished after {} timesteps.'.format(t+1))\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    return np.random.choice(3, size=((10,10,10)))\n",
    "\n",
    "def crossover(policy1, policy2, grid_size=10):\n",
    "    new_policy = policy1.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            for k in range(10):\n",
    "                rand = np.random.uniform()\n",
    "                if rand > 0.5:\n",
    "                    new_policy[i][j][k] = policy2[i][j][k]\n",
    "    return new_policy\n",
    "\n",
    "def mutation(policy, p=0.05):\n",
    "    new_policy = policy.copy()\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            for k in range(10):\n",
    "                rand = np.random.uniform()\n",
    "                if rand < p:\n",
    "                    new_policy[i] = np.random.choice(4)\n",
    "    return new_policy\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "     Returns three initialized objects: the model, the environment, and the replay.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.2), \"mse\")\n",
    "\n",
    "    # Define environment/game\n",
    "    env = Catch()\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "    \n",
    "    return model, env, exp_replay\n",
    "\n",
    "def create_animation(model, env, num_games):\n",
    "    '''\n",
    "    Inputs:\n",
    "        model and env objects as returned from build_model\n",
    "        num_games: integer, the number of games to be included in the animation\n",
    "        \n",
    "    Returns: a matplotlib animation object\n",
    "    '''\n",
    "    # Animation code from \n",
    "    # https://matplotlib.org/examples/animation/dynamic_image.html\n",
    "    # https://stackoverflow.com/questions/35532498/animation-in-ipython-notebook/46878531#46878531\n",
    "    \n",
    "    # First, play the games and collect all of the images for each observed state\n",
    "    observations = []\n",
    "    for _ in range(num_games):\n",
    "        env.reset()\n",
    "        observation = env.observe()\n",
    "        observations.append(observation)\n",
    "        game_over = False\n",
    "        while game_over == False:\n",
    "            q = model.predict(observation)\n",
    "            action = np.argmax(q[0])\n",
    "            \n",
    "            # apply action, get rewards and new state\n",
    "            observation, reward, game_over = env.act(policy)\n",
    "            observations.append(observation)\n",
    "            \n",
    "    fig = plt.figure()\n",
    "    image = plt.imshow(np.zeros((grid_size, grid_size)),interpolation='none', cmap='gray', animated=True, vmin=0, vmax=1)\n",
    "    \n",
    "    def animate(observation):\n",
    "        image.set_array(observation.reshape((grid_size, grid_size)))\n",
    "        return [image]\n",
    "   \n",
    "    animation = matplotlib.animation.FuncAnimation(fig, animate, frames=observations, blit=True, )\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 : max score = -0.02\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid action 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-282c13a2513d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmutated_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mpolicy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mbest_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-282c13a2513d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melite_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpolicy_pop\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmutated_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mpolicy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mbest_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_pop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ba8b85610493>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(env, policy, n_episodes)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ba8b85610493>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, policy, grid_size, episode_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfruit_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfruit_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasket_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fe973136190e>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#         print(\"Action:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#         print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fe973136190e>\u001b[0m in \u001b[0;36m_update_state\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmovement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid action {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mfruit_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfruit_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasket_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# move the basket unless this would move it off the edge of the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Invalid action 3"
     ]
    }
   ],
   "source": [
    "random.seed(4904) # we kick bot\n",
    "np.random.seed(4904)\n",
    "model, env, exp_replay = build_model()\n",
    "## Policy search\n",
    "n_policy = 100\n",
    "n_steps = 1\n",
    "# start = time.time()\n",
    "\n",
    "# note to self: the underscore can be used as a throwaway value when iterating through something,\n",
    "# like when you want to do something x times but don't care about the value of x\n",
    "policy_pop = [gen_random_policy() for _ in range(n_policy)]\n",
    "\n",
    "env.reset()\n",
    "env.state\n",
    "policy_pop\n",
    "test_policy = policy_pop[0]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    policy_scores = [evaluate_policy(env, p) for p in policy_pop]\n",
    "    print('Generation %d : max score = %0.2f' %(idx+1, max(policy_scores)))\n",
    "    policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "    elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "    select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
    "    child_set = [crossover(\n",
    "        policy_pop[np.random.choice(range(n_policy), p=select_probs)],\n",
    "        policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n",
    "        for _ in range(n_policy - 5)]\n",
    "    mutated_list = [mutation(p) for p in child_set]\n",
    "    policy_pop = elite_set\n",
    "    policy_pop += mutated_list\n",
    "policy_score = [evaluate_policy(env, p) for p in policy_pop]\n",
    "best_policy = policy_pop[np.argmax(policy_score)]\n",
    "\n",
    "end = time.time()\n",
    "print('Best policy score = %0.2f.'\n",
    "        %(np.max(policy_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_policy[0][0][0]\n",
    "a = env.act(test_policy)\n",
    "fig = plt.figure()\n",
    "image = plt.imshow(np.zeros((grid_size, grid_size)),interpolation='none', cmap='gray', animated=True, vmin=0, vmax=1)\n",
    "\n",
    "def animate(observation):\n",
    "    image.set_array(observation.reshape((grid_size, grid_size)))\n",
    "    return [image]\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, frames=a, blit=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
