{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling1D, Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_len = 20\n",
    "num_words = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(num_words):\n",
    "    X_train = pd.read_csv(\"/Users/isaanca/MachineLearning/fall/better_quest_data.csv\", engine=\"python\")\n",
    "    X_train = X_train.rename(index=str, columns={\n",
    "        \"TITLE of your Quest Project (for Program)\": \"Title\", \n",
    "        \"DISCIPLINE(S) of YOUR QUEST (for Program)\": \"Disciplines\"\n",
    "    })\n",
    "    X_train = [i for i in X_train[\"Title\"]]\n",
    "    \n",
    "    # for the purpose of this model, I am disregarding the capitalization of titles\n",
    "    X_train = [i.lower() for i in X_train]\n",
    "    \n",
    "    word_vector = CountVectorizer(token_pattern=r\"((\\w|'|’)+)\", strip_accents=\"ascii\")\n",
    "    \n",
    "    term_freq = word_vector.fit_transform(X_train)\n",
    "    word_index = word_vector.vocabulary_\n",
    "    word_index = {key[0]:value for key, value in word_index.items()}\n",
    "    word_index[\"PADDING\"] = len(word_index)\n",
    "    word_index[\"END\"] = len(word_index)\n",
    "    num_classes = len(word_index)\n",
    "    \n",
    "    # the reverse index allows the numbers to be converted back into words\n",
    "    reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "    \n",
    "    # splits the words, using the same token pattern as the count vectorizer\n",
    "    def split_words(title):\n",
    "        return [i[0] for i in re.findall(r\"((\\w|'|’)+)\", title.lower())]\n",
    "\n",
    "    def pad_data(data, cutoff):\n",
    "        new_X_train = data\n",
    "        for i in range(len(new_X_train)):\n",
    "            title = new_X_train[i]\n",
    "            title = title[0:(cutoff-1)]\n",
    "            while len(title) < (cutoff-1):\n",
    "                title.append(\"PADDING\")\n",
    "            title.append(\"END\")\n",
    "            new_X_train[i] = title\n",
    "        return new_X_train\n",
    "\n",
    "    def index_words(word_list, word_index):\n",
    "        return [word_index[i] for i in word_list]\n",
    "\n",
    "    X_train = list(map(split_words, X_train))\n",
    "    X_train = pad_data(X_train, num_words)\n",
    "    X_train = [index_words(i, word_index) for i in X_train]\n",
    "    \n",
    "    X_train = np.array([np.transpose(to_categorical(X_train, num_classes=num_classes)[i]) for i in range(len(X_train))])\n",
    "    \n",
    "    return(X_train, reverse_word_index, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n",
      "847\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "X_train, vocab, num_classes = process_data(num_words)\n",
    "# print(test)\n",
    "# print(X_train)\n",
    "\n",
    "print(len(X_train)) # number of data points/Quest titles\n",
    "print(len(X_train[0])) # number of words in vocabulary\n",
    "print(len(X_train[0][0])) # number of words per title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(num_classes, num_words):\n",
    "    '''\n",
    "    Put together a CNN that will return a single confidence output.\n",
    "    \n",
    "    returns: the model object\n",
    "    '''\n",
    "    \n",
    "    # dimensions: rows = list of words, columns/channels = # of words per title\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=num_words, strides=2, input_shape=(num_classes, num_words), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, kernel_size=num_words, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(128, kernel_size=num_words, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(256, kernel_size=num_words, strides=1, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(num_classes, num_words):\n",
    "    '''\n",
    "    Put together a model that takes in one-dimensional noise and outputs two-dimensional\n",
    "    data representing a three-word phrase.\n",
    "    \n",
    "    returns: the model object\n",
    "    '''\n",
    "    \n",
    "    # note to self: trailing commas are used for single-element tuples\n",
    "    noise_shape = (noise_len,)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_classes * num_words, activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape((num_classes, num_words)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(128, kernel_size=num_words, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8)) \n",
    "    model.add(Conv1D(64, kernel_size=num_words, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv1D(num_words, kernel_size=num_words, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined(num_classes, num_words):\n",
    "    '''\n",
    "    Puts together a model that combines the discriminator and generator models.\n",
    "    \n",
    "    returns: the generator, discriminator, and combined model objects\n",
    "    '''\n",
    "    \n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator(num_classes, num_words)\n",
    "    discriminator.compile(loss='binary_crossentropy', \n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Build and compile the generator\n",
    "    generator = build_generator(num_classes, num_words)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # The generator takes noise as input and generates images\n",
    "    noise = Input(shape=(noise_len,))\n",
    "    title = generator(noise)\n",
    "    \n",
    "    \n",
    "    # For the combined model we will only train the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated images as input and determines validity\n",
    "    valid = discriminator(title)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity \n",
    "    combined = Model(inputs=noise, outputs=valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator, discriminator, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_titles(generator, epoch, vocab):\n",
    "    '''\n",
    "    Has the generator create and save new Quest titles.\n",
    "    \n",
    "    inputs:\n",
    "        generator: the generator model object returned by build_combined\n",
    "        epoch: the epoch number (but can be anything that can be represented as a string)\n",
    "        vocab: the mapping of numbers to words\n",
    "    \n",
    "    returns: None\n",
    "    '''\n",
    "    titles = 1\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (titles, noise_len))\n",
    "    gen_title = generator.predict(noise)\n",
    "    \n",
    "    # chooses the word with the highest weight\n",
    "    gen_title = [np.argmax([j[i] for j in gen_title[0]]) for i in range(8)]\n",
    "    \n",
    "    # map words to numbers\n",
    "    gen_title = \" \".join([vocab[i] for i in gen_title])\n",
    "    \n",
    "    file=open('titles_real_data/titles_{}.txt'.format(epoch),\"w+\")\n",
    "    file.write(gen_title)\n",
    "    print(gen_title)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, combined, data, vocab, epochs, batch_size=128, save_interval=50):\n",
    "    '''\n",
    "    Trains all model objects\n",
    "    \n",
    "    generator: the generator model object returned by build_combined\n",
    "    discriminator: the discriminator model object returned by build_combined\n",
    "    combined: the combined model object returned by build_combined\n",
    "    epochs: integer, the number of epochs to train for\n",
    "    batch_size: integer, the number of training samples to use at a time\n",
    "    save_interval: integer, will generate and save images when the current epoch % save_interval is 0\n",
    "    \n",
    "    returns: None\n",
    "    '''\n",
    "\n",
    "    # Load the dataset\n",
    "    X_train = data\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        titles = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a half batch of new titles\n",
    "        noise = np.random.normal(0, 1, (half_batch, noise_len))\n",
    "        gen_titles = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(titles, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_titles, np.zeros((half_batch, 1)))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_len))\n",
    "        # Train the generator (wants discriminator to mistake titles as real)\n",
    "        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "           \n",
    "        # If at save interval => save generated image samples and plot progress\n",
    "        if epoch % save_interval == 0:\n",
    "            # Plot the progress\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            print(\"{} [D loss: {}, acc.: {:.2%}] [G loss: {}]\".format(epoch, d_loss[0], d_loss[1], g_loss))\n",
    "            save_titles(generator, epoch, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, discriminator, combined = build_combined(num_classes, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.6125308871269226, acc.: 68.75%] [G loss: 1.0838741064071655]\n",
      "sneakers boom sandpiper personal choreography hyperplasia rebuilding neuroscience\n",
      "200 [D loss: 0.005813055671751499, acc.: 100.00%] [G loss: 7.390000343322754]\n",
      "algorithms world world airship ableton adventures PADDING END\n",
      "400 [D loss: 4.2221604417136405e-06, acc.: 100.00%] [G loss: 16.047252655029297]\n",
      "nation note controlled you PADDING PADDING PADDING END\n",
      "600 [D loss: 0.007651554420590401, acc.: 100.00%] [G loss: 14.232666969299316]\n",
      "END zer0 PADDING PADDING PADDING PADDING PADDING END\n",
      "800 [D loss: 0.02080383338034153, acc.: 100.00%] [G loss: 8.97913932800293]\n",
      "END youth youtube zer0 PADDING PADDING PADDING END\n",
      "1000 [D loss: 3.171868956997059e-05, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "classical architecture clinic PADDING replacement PADDING PADDING END\n",
      "1200 [D loss: 3.914362907409668, acc.: 53.12%] [G loss: 16.11809539794922]\n",
      "mom's youtube modern gap PADDING PADDING PADDING END\n",
      "1400 [D loss: 2.826281706802547e-05, acc.: 100.00%] [G loss: 16.10479736328125]\n",
      "youtube END PADDING PADDING youtube PADDING PADDING END\n",
      "1600 [D loss: 4.2848729208344594e-05, acc.: 100.00%] [G loss: 16.106958389282227]\n",
      "you END PADDING PADDING youtube END PADDING END\n",
      "1800 [D loss: 2.9732556868111715e-05, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "modern modeling mom's PADDING multicourse END PADDING END\n",
      "2000 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.07274627685547]\n",
      "1944 a 1978 4 4 about PADDING END\n",
      "2200 [D loss: 1.1255534815290957e-07, acc.: 100.00%] [G loss: 14.17253303527832]\n",
      "1 1978 1978 PADDING PADDING 3 PADDING 1\n",
      "2400 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "mom's movie youtube PADDING PADDING PADDING PADDING END\n",
      "2600 [D loss: 1.1332996052715316e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "1 1978 1978 PADDING PADDING 3 PADDING END\n",
      "2800 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "engaging english youtube END PADDING PADDING PADDING END\n",
      "3000 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "1 a 1978 PADDING academia 3 PADDING END\n",
      "3200 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "1 1978 1978 END activated 3 PADDING 3\n",
      "3400 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "PADDING youtube youtube PADDING PADDING PADDING PADDING END\n",
      "3600 [D loss: 5.007625532016391e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "youtube zer0 youtube END PADDING PADDING PADDING END\n",
      "3800 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "youtube zer0 youtube END PADDING PADDING PADDING END\n",
      "4000 [D loss: 1.0960467022869125e-07, acc.: 100.00%] [G loss: 16.11809539794922]\n",
      "map market increasing PADDING PADDING PADDING PADDING END\n"
     ]
    }
   ],
   "source": [
    "train(generator, discriminator, combined, X_train, vocab, epochs=4001, batch_size=32, save_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so my model really likes moms and youtube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6New",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
